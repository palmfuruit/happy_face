モデル

https://huggingface.co/motheecreator/vit-Facial-Expression-Recognition



このモデル「vit-Facial-Expression-Recognition」は、Hugging Face上に公開されているファインチューニングされたVision Transformer（ViT）モデルで、主に顔の表情認識に使用されます。以下に、このモデルの詳細を解説します。

1. モデルの概要
モデル名: vit-Facial-Expression-Recognition
ベースモデル: google/vit-base-patch16-224-in21k
ViT（Vision Transformer）ベースのモデルで、画像分類などのタスクに使われる画像処理モデル。
タスク: 顔の表情認識
入力画像を用いて、人の表情を7つの感情（怒り、嫌悪、恐怖、幸福、悲しみ、驚き、中立）に分類します。
トレーニングデータセット: FER2013、MMI Facial Expression、AffectNet
これらのデータセットは、表情認識用にラベル付けされた顔の画像が含まれています。
2. データの前処理
リサイズ: 入力画像は指定されたサイズにリサイズされます（224x224ピクセル）。
正規化: 画像のピクセル値は、ニューラルネットワークに適した範囲に正規化されます。
データ拡張: トレーニングデータセットを拡張するために、ランダムな回転、反転、ズームなどの変換が適用されます。これにより、モデルの汎化性能が向上します。
3. トレーニングのハイパーパラメータ
学習率: 3e-05
バッチサイズ: 32
勾配蓄積ステップ: 8
実際のトレーニングのバッチサイズは、256（32バッチサイズ × 勾配蓄積ステップ8）に相当します。
オプティマイザ: Adam (betas=(0.9,0.999), epsilon=1e-08)
学習率スケジューラ: コサインスケジューラ
ウォームアップステップ: 1000
エポック数: 3
4. トレーニング結果
損失値（Loss）: トレーニングが進むにつれて、損失は初期の1.3548から最終的に0.4503まで低下しています。
精度（Accuracy）: 精度は最初の0.7418から徐々に向上し、最終的には0.8434に達しています。
トレーニングデータセットを使って、感情認識のタスクに適したモデルに仕上がっています。
5. 使用するフレームワーク
Transformers: 4.36.0
PyTorch: 2.0.0
Datasets: 2.1.0
Tokenizers: 0.15.0
このモデルは、Vision Transformer（ViT）アーキテクチャをベースにしており、表情認識タスクにおいて高精度を達成しています。トレーニングに使われたデータセットは多様で、一般的な顔の感情分類タスクに適しています。また、データ拡張や学習率の調整などの工夫により、モデルの汎化性能が向上しています。